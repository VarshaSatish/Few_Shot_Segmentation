{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Varsha_Seminar_PANet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wux2BXu6uvmr"
      },
      "source": [
        "d = []\n",
        "while(1):\n",
        "  d.append(1)\n",
        "\n",
        "# function ConnectButton(){\n",
        "#    console.log(\"Connect pushed\"); \n",
        "#    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "# }\n",
        "# setInterval(ConnectButton,1800000);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKE2NSW2NwUY"
      },
      "source": [
        "mode = 'train' # 'train' or 'test'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaXbs_os6bEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3b0062-b2d8-45e3-e3ff-288d0ebca887"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJRGbf9MJajb"
      },
      "source": [
        "**Initializations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_mEPwE_d05J"
      },
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as tr_F\n",
        "import os\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch.optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchvision.transforms import Compose\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twbjV99dJje2"
      },
      "source": [
        "**Transformations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e06Qk6z5iuIB"
      },
      "source": [
        "\"\"\"\n",
        "Customized data transforms\n",
        "\"\"\"\n",
        "#dataloaders.transforms.py\n",
        "\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms.functional as tr_F\n",
        "\n",
        "\n",
        "class RandomMirror(object):\n",
        "    \"\"\"\n",
        "    Randomly filp the images/masks horizontally\n",
        "    \"\"\"\n",
        "    def __call__(self, sample):\n",
        "        img, label = sample['image'], sample['label']\n",
        "        inst, scribble = sample['inst'], sample['scribble']\n",
        "        if random.random() < 0.5:\n",
        "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            if isinstance(label, dict):\n",
        "                label = {catId: x.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                         for catId, x in label.items()}\n",
        "            else:\n",
        "                label = label.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            inst = inst.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            scribble = scribble.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "        sample['image'] = img\n",
        "        sample['label'] = label\n",
        "        sample['inst'] = inst\n",
        "        sample['scribble'] = scribble\n",
        "        return sample\n",
        "\n",
        "class Resize(object):\n",
        "    \"\"\"\n",
        "    Resize images/masks to given size\n",
        "\n",
        "    Args:\n",
        "        size: output size\n",
        "    \"\"\"\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        img, label = sample['image'], sample['label']\n",
        "        inst, scribble = sample['inst'], sample['scribble']\n",
        "        img = tr_F.resize(img, self.size)\n",
        "        if isinstance(label, dict):\n",
        "            label = {catId: tr_F.resize(x, self.size, interpolation=Image.NEAREST)\n",
        "                     for catId, x in label.items()}\n",
        "        else:\n",
        "            label = tr_F.resize(label, self.size, interpolation=Image.NEAREST)\n",
        "        inst = tr_F.resize(inst, self.size, interpolation=Image.NEAREST)\n",
        "        scribble = tr_F.resize(scribble, self.size, interpolation=Image.ANTIALIAS)\n",
        "\n",
        "        sample['image'] = img\n",
        "        sample['label'] = label\n",
        "        sample['inst'] = inst\n",
        "        sample['scribble'] = scribble\n",
        "        return sample\n",
        "\n",
        "class DilateScribble(object):\n",
        "    \"\"\"\n",
        "    Dilate the scribble mask\n",
        "\n",
        "    Args:\n",
        "        size: window width\n",
        "    \"\"\"\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        scribble = sample['scribble']\n",
        "        dilated_scribble = Image.fromarray(\n",
        "            ndimage.minimum_filter(np.array(scribble), size=self.size))\n",
        "        dilated_scribble.putpalette(scribble.getpalette())\n",
        "\n",
        "        sample['scribble'] = dilated_scribble\n",
        "        return sample\n",
        "\n",
        "class ToTensorNormalize(object):\n",
        "    \"\"\"\n",
        "    Convert images/masks to torch.Tensor\n",
        "    Scale images' pixel values to [0-1] and normalize with predefined statistics\n",
        "    \"\"\"\n",
        "    def __call__(self, sample):\n",
        "        img, label = sample['image'], sample['label']\n",
        "        inst, scribble = sample['inst'], sample['scribble']\n",
        "        img = tr_F.to_tensor(img)\n",
        "        img = tr_F.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        if isinstance(label, dict):\n",
        "            label = {catId: torch.Tensor(np.array(x)).long()\n",
        "                     for catId, x in label.items()}\n",
        "        else:\n",
        "            label = torch.Tensor(np.array(label)).long()\n",
        "        inst = torch.Tensor(np.array(inst)).long()\n",
        "        scribble = torch.Tensor(np.array(scribble)).long()\n",
        "\n",
        "        sample['image'] = img\n",
        "        sample['label'] = label\n",
        "        sample['inst'] = inst\n",
        "        sample['scribble'] = scribble\n",
        "        return sample\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpKdd65tJvyu"
      },
      "source": [
        "**Paired Dataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9iwUxOBJTQS"
      },
      "source": [
        "# To create paired dataset\n",
        "\n",
        "class BaseDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Base Dataset\n",
        "    Args:\n",
        "        base_dir:\n",
        "            dataset directory\n",
        "    \"\"\"\n",
        "    def __init__(self, base_dir):\n",
        "        self._base_dir = base_dir\n",
        "        self.aux_attrib = {}\n",
        "        self.aux_attrib_args = {}\n",
        "        self.ids = []  # must be overloaded in subclass\n",
        "\n",
        "    def add_attrib(self, key, func, func_args):\n",
        "        \"\"\"\n",
        "        Add attribute to the data sample dict\n",
        "        Args:\n",
        "            key:\n",
        "                key in the data sample dict for the new attribute\n",
        "                e.g. sample['click_map'], sample['depth_map']\n",
        "            func:\n",
        "                function to process a data sample and create an attribute (e.g. user clicks)\n",
        "            func_args:\n",
        "                extra arguments to pass, expected a dict\n",
        "        \"\"\"\n",
        "        if key in self.aux_attrib:\n",
        "            raise KeyError(\"Attribute '{0}' already exists, please use 'set_attrib'.\".format(key))\n",
        "        else:\n",
        "            self.set_attrib(key, func, func_args)\n",
        "\n",
        "    def set_attrib(self, key, func, func_args):\n",
        "        \"\"\"\n",
        "        Set attribute in the data sample dict\n",
        "        Args:\n",
        "            key:\n",
        "                key in the data sample dict for the new attribute\n",
        "                e.g. sample['click_map'], sample['depth_map']\n",
        "            func:\n",
        "                function to process a data sample and create an attribute (e.g. user clicks)\n",
        "            func_args:\n",
        "                extra arguments to pass, expected a dict\n",
        "        \"\"\"\n",
        "        self.aux_attrib[key] = func\n",
        "        self.aux_attrib_args[key] = func_args\n",
        "\n",
        "    def del_attrib(self, key):\n",
        "        \"\"\"\n",
        "        Remove attribute in the data sample dict\n",
        "        Args:\n",
        "            key:\n",
        "                key in the data sample dict\n",
        "        \"\"\"\n",
        "        self.aux_attrib.pop(key)\n",
        "        self.aux_attrib_args.pop(key)\n",
        "\n",
        "    def subsets(self, sub_ids, sub_args_lst=None):\n",
        "        \"\"\"\n",
        "        Create subsets by ids\n",
        "        Args:\n",
        "            sub_ids:\n",
        "                a sequence of sequences, each sequence contains data ids for one subset\n",
        "            sub_args_lst:\n",
        "                a list of args for some subset-specific auxiliary attribute function\n",
        "        \"\"\"\n",
        "\n",
        "        indices = [[self.ids.index(id_) for id_ in ids] for ids in sub_ids]\n",
        "        if sub_args_lst is not None:\n",
        "            subsets = [Subset(dataset=self, indices=index, sub_attrib_args=args)\n",
        "                       for index, args in zip(indices, sub_args_lst)]\n",
        "        else:\n",
        "            subsets = [Subset(dataset=self, indices=index) for index in indices]\n",
        "        return subsets\n",
        "\n",
        "\n",
        "class PairedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Make pairs of data from dataset\n",
        "    When 'same=True',\n",
        "        a pair contains data from same datasets,\n",
        "        and the choice of datasets for each pair is random.\n",
        "        e.g. [[ds1_3, ds1_2], [ds3_1, ds3_2], [ds2_1, ds2_2], ...]\n",
        "    When 'same=False',\n",
        "            a pair contains data from different datasets,\n",
        "            if 'n_elements' <= # of datasets, then we randomly choose a subset of datasets,\n",
        "                then randomly choose a sample from each dataset in the subset\n",
        "                e.g. [[ds1_3, ds2_1, ds3_1], [ds4_1, ds2_3, ds3_2], ...]\n",
        "            if 'n_element' is a list of int, say [C_1, C_2, C_3, ..., C_k], we first\n",
        "                randomly choose k(k < # of datasets) datasets, then draw C_1, C_2, ..., C_k samples\n",
        "                from each dataset respectively.\n",
        "                Note the total number of elements will be (C_1 + C_2 + ... + C_k).\n",
        "    Args:\n",
        "        datasets:\n",
        "            source datasets, expect a list of Dataset\n",
        "        n_elements:\n",
        "            number of elements in a pair\n",
        "        max_iters:\n",
        "            number of pairs to be sampled\n",
        "        same:\n",
        "            whether data samples in a pair are from the same dataset or not,\n",
        "            see a detailed explanation above.\n",
        "        pair_based_transforms:\n",
        "            some transformation performed on a pair basis, expect a list of functions,\n",
        "            each function takes a pair sample and return a transformed one.\n",
        "    \"\"\"\n",
        "    def __init__(self, datasets, n_elements, max_iters, same=True,\n",
        "                 pair_based_transforms=None):\n",
        "        super().__init__()\n",
        "        self.datasets = datasets\n",
        "        self.n_datasets = len(self.datasets)\n",
        "        self.n_data = [len(dataset) for dataset in self.datasets]\n",
        "        self.n_elements = n_elements\n",
        "        self.max_iters = max_iters\n",
        "        self.pair_based_transforms = pair_based_transforms\n",
        "        if same:\n",
        "            if isinstance(self.n_elements, int):\n",
        "                datasets_indices = [random.randrange(self.n_datasets)\n",
        "                                    for _ in range(self.max_iters)]\n",
        "                self.indices = [[(dataset_idx, data_idx)\n",
        "                                 for data_idx in random.choices(range(self.n_data[dataset_idx]),\n",
        "                                                                k=self.n_elements)]\n",
        "                                for dataset_idx in datasets_indices]\n",
        "            else:\n",
        "                raise ValueError(\"When 'same=true', 'n_element' should be an integer.\")\n",
        "        else:\n",
        "            if isinstance(self.n_elements, list):\n",
        "                self.indices = [[(dataset_idx, data_idx)\n",
        "                                 for i, dataset_idx in enumerate(\n",
        "                                     random.sample(range(self.n_datasets), k=len(self.n_elements)))\n",
        "                                 for data_idx in random.sample(range(self.n_data[dataset_idx]),\n",
        "                                                               k=self.n_elements[i])]\n",
        "                                for i_iter in range(self.max_iters)]\n",
        "            elif self.n_elements > self.n_datasets:\n",
        "                raise ValueError(\"When 'same=False', 'n_element' should be no more than n_datasets\")\n",
        "            else:\n",
        "                self.indices = [[(dataset_idx, random.randrange(self.n_data[dataset_idx]))\n",
        "                                 for dataset_idx in random.sample(range(self.n_datasets),\n",
        "                                                                  k=n_elements)]\n",
        "                                for i in range(max_iters)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.max_iters\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = [self.datasets[dataset_idx][data_idx]\n",
        "                  for dataset_idx, data_idx in self.indices[idx]]\n",
        "        if self.pair_based_transforms is not None:\n",
        "            for transform, args in self.pair_based_transforms:\n",
        "                sample = transform(sample, **args)\n",
        "        return sample\n",
        "\n",
        "\n",
        "class Subset(Dataset):\n",
        "    \"\"\"\n",
        "    Subset of a dataset at specified indices.\n",
        "    Args:\n",
        "        dataset:\n",
        "            The whole Dataset\n",
        "        indices:\n",
        "            Indices in the whole set selected for subset\n",
        "        sub_attrib_args:\n",
        "            Subset-specific arguments for attribute functions, expected a dict\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, indices, sub_attrib_args=None):\n",
        "        self.dataset = dataset\n",
        "        self.indices = indices\n",
        "        self.sub_attrib_args = sub_attrib_args\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.sub_attrib_args is not None:\n",
        "            for key in self.sub_attrib_args:\n",
        "                # Make sure the dataset already has the corresponding attributes\n",
        "                # Here we only make the arguments subset dependent\n",
        "                #   (i.e. pass different arguments for each subset)\n",
        "                self.dataset.aux_attrib_args[key].update(self.sub_attrib_args[key])\n",
        "        return self.dataset[self.indices[idx]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvHqF4IJJq_3"
      },
      "source": [
        "**Dataset Loading**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlCIW667d8nW"
      },
      "source": [
        "\"\"\"\n",
        "Load pascal VOC dataset\n",
        "\"\"\"\n",
        "#pascal.py\n",
        "class VOC(BaseDataset):\n",
        "    \"\"\"\n",
        "    Base Class for VOC Dataset\n",
        "\n",
        "    Args:\n",
        "        base_dir:\n",
        "            VOC dataset directory\n",
        "        split:\n",
        "            which split to use\n",
        "            choose from ('train', 'val', 'trainval', 'trainaug')\n",
        "        transform:\n",
        "            transformations to be performed on images/masks\n",
        "        to_tensor:\n",
        "            transformation to convert PIL Image to tensor\n",
        "    \"\"\"\n",
        "    def __init__(self, base_dir, split, transforms=None, to_tensor=None):\n",
        "        super().__init__(base_dir)\n",
        "        self.split = split\n",
        "        self._image_dir = os.path.join(self._base_dir, 'JPEGImages')\n",
        "        self._label_dir = os.path.join(self._base_dir, 'SegmentationClassAug')\n",
        "        self._inst_dir = os.path.join(self._base_dir, 'SegmentationObjectAug')\n",
        "        self._scribble_dir = os.path.join(self._base_dir, 'ScribbleAugAuto')\n",
        "        self._id_dir = os.path.join(self._base_dir, 'ImageSets', 'Segmentation')\n",
        "        self.transforms = transforms\n",
        "        self.to_tensor = to_tensor\n",
        "\n",
        "        with open(os.path.join(self._id_dir, f'{self.split}.txt'), 'r') as f:\n",
        "            self.ids = f.read().splitlines()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Fetch data\n",
        "        id_ = self.ids[idx]\n",
        "        image = Image.open(os.path.join(self._image_dir, f'{id_}.jpg'))\n",
        "        semantic_mask = Image.open(os.path.join(self._label_dir, f'{id_}.png'))\n",
        "        instance_mask = Image.open(os.path.join(self._inst_dir, f'{id_}.png'))\n",
        "        scribble_mask = Image.open(os.path.join(self._scribble_dir, f'{id_}.png'))\n",
        "        sample = {'image': image,\n",
        "                  'label': semantic_mask,\n",
        "                  'inst': instance_mask,\n",
        "                  'scribble': scribble_mask}\n",
        "\n",
        "        # Image-level transformation\n",
        "        if self.transforms is not None:\n",
        "            sample = self.transforms(sample)\n",
        "        # Save the original image (without normalization)\n",
        "        image_t = torch.from_numpy(np.array(sample['image']).transpose(2, 0, 1))\n",
        "        # Transform to tensor\n",
        "        if self.to_tensor is not None:\n",
        "            sample = self.to_tensor(sample)\n",
        "\n",
        "        sample['id'] = id_\n",
        "        sample['image_t'] = image_t\n",
        "\n",
        "        # Add auxiliary attributes\n",
        "        for key_prefix in self.aux_attrib:\n",
        "            # Process the data sample, create new attributes and save them in a dictionary\n",
        "            aux_attrib_val = self.aux_attrib[key_prefix](sample, **self.aux_attrib_args[key_prefix])\n",
        "            for key_suffix in aux_attrib_val:\n",
        "                # one function may create multiple attributes, so we need suffix to distinguish them\n",
        "                sample[key_prefix + '_' + key_suffix] = aux_attrib_val[key_suffix]\n",
        "\n",
        "        return sample\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp1mn22_J4pl"
      },
      "source": [
        "**Creating the specific customized dataset for the task**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kHeA6qp7e1V"
      },
      "source": [
        "\"\"\"\n",
        "Customized dataset\n",
        "\"\"\"\n",
        "#custom.py\n",
        "def attrib_basic(_sample, class_id):\n",
        "    \"\"\"\n",
        "    Add basic attribute\n",
        "\n",
        "    Args:\n",
        "        _sample: data sample\n",
        "        class_id: class label asscociated with the data\n",
        "            (sometimes indicting from which subset the data are drawn)\n",
        "    \"\"\"\n",
        "    return {'class_id': class_id}\n",
        "\n",
        "\n",
        "def getMask(label, scribble, class_id, class_ids):\n",
        "    \"\"\"\n",
        "    Generate FG/BG mask from the segmentation mask\n",
        "\n",
        "    Args:\n",
        "        label:\n",
        "            semantic mask\n",
        "        scribble:\n",
        "            scribble mask\n",
        "        class_id:\n",
        "            semantic class of interest\n",
        "        class_ids:\n",
        "            all class id in this episode\n",
        "    \"\"\"\n",
        "    # Dense Mask\n",
        "    fg_mask = torch.where(label == class_id,\n",
        "                          torch.ones_like(label), torch.zeros_like(label))\n",
        "    bg_mask = torch.where(label != class_id,\n",
        "                          torch.ones_like(label), torch.zeros_like(label))\n",
        "    for class_id in class_ids:\n",
        "        bg_mask[label == class_id] = 0\n",
        "\n",
        "    # Scribble Mask\n",
        "    bg_scribble = scribble == 0\n",
        "    fg_scribble = torch.where((fg_mask == 1)\n",
        "                              & (scribble != 0)\n",
        "                              & (scribble != 255),\n",
        "                              scribble, torch.zeros_like(fg_mask))\n",
        "    scribble_cls_list = list(set(np.unique(fg_scribble)) - set([0,]))\n",
        "    if scribble_cls_list:  # Still need investigation\n",
        "        fg_scribble = fg_scribble == random.choice(scribble_cls_list).item()\n",
        "    else:\n",
        "        fg_scribble[:] = 0\n",
        "\n",
        "    return {'fg_mask': fg_mask,\n",
        "            'bg_mask': bg_mask,\n",
        "            'fg_scribble': fg_scribble.long(),\n",
        "            'bg_scribble': bg_scribble.long()}\n",
        "\n",
        "\n",
        "def fewShot(paired_sample, n_ways, n_shots, cnt_query, coco=False):\n",
        "    \"\"\"\n",
        "    Postprocess paired sample for fewshot settings\n",
        "\n",
        "    Args:\n",
        "        paired_sample:\n",
        "            data sample from a PairedDataset\n",
        "        n_ways:\n",
        "            n-way few-shot learning\n",
        "        n_shots:\n",
        "            n-shot few-shot learning\n",
        "        cnt_query:\n",
        "            number of query images for each class in the support set\n",
        "        coco:\n",
        "            MS COCO dataset\n",
        "    \"\"\"\n",
        "    ###### Compose the support and query image list ######\n",
        "    cumsum_idx = np.cumsum([0,] + [n_shots + x for x in cnt_query])\n",
        "\n",
        "    # support class ids\n",
        "    class_ids = [paired_sample[cumsum_idx[i]]['basic_class_id'] for i in range(n_ways)]\n",
        "\n",
        "    # support images\n",
        "    support_images = [[paired_sample[cumsum_idx[i] + j]['image'] for j in range(n_shots)]\n",
        "                      for i in range(n_ways)]\n",
        "    support_images_t = [[paired_sample[cumsum_idx[i] + j]['image_t'] for j in range(n_shots)]\n",
        "                        for i in range(n_ways)]\n",
        "\n",
        "    # support image labels\n",
        "    if coco:\n",
        "        support_labels = [[paired_sample[cumsum_idx[i] + j]['label'][class_ids[i]]\n",
        "                           for j in range(n_shots)] for i in range(n_ways)]\n",
        "    else:\n",
        "        support_labels = [[paired_sample[cumsum_idx[i] + j]['label'] for j in range(n_shots)]\n",
        "                          for i in range(n_ways)]\n",
        "    support_scribbles = [[paired_sample[cumsum_idx[i] + j]['scribble'] for j in range(n_shots)]\n",
        "                         for i in range(n_ways)]\n",
        "    support_insts = [[paired_sample[cumsum_idx[i] + j]['inst'] for j in range(n_shots)]\n",
        "                     for i in range(n_ways)]\n",
        "\n",
        "\n",
        "\n",
        "    # query images, masks and class indices\n",
        "    query_images = [paired_sample[cumsum_idx[i+1] - j - 1]['image'] for i in range(n_ways)\n",
        "                    for j in range(cnt_query[i])]\n",
        "    query_images_t = [paired_sample[cumsum_idx[i+1] - j - 1]['image_t'] for i in range(n_ways)\n",
        "                      for j in range(cnt_query[i])]\n",
        "    if coco:\n",
        "        query_labels = [paired_sample[cumsum_idx[i+1] - j - 1]['label'][class_ids[i]]\n",
        "                        for i in range(n_ways) for j in range(cnt_query[i])]\n",
        "    else:\n",
        "        query_labels = [paired_sample[cumsum_idx[i+1] - j - 1]['label'] for i in range(n_ways)\n",
        "                        for j in range(cnt_query[i])]\n",
        "    query_cls_idx = [sorted([0,] + [class_ids.index(x) + 1\n",
        "                                    for x in set(np.unique(query_label)) & set(class_ids)])\n",
        "                     for query_label in query_labels]\n",
        "\n",
        "\n",
        "    ###### Generate support image masks ######\n",
        "    support_mask = [[getMask(support_labels[way][shot], support_scribbles[way][shot],\n",
        "                             class_ids[way], class_ids)\n",
        "                     for shot in range(n_shots)] for way in range(n_ways)]\n",
        "\n",
        "\n",
        "    ###### Generate query label (class indices in one episode, i.e. the ground truth)######\n",
        "    query_labels_tmp = [torch.zeros_like(x) for x in query_labels]\n",
        "    for i, query_label_tmp in enumerate(query_labels_tmp):\n",
        "        query_label_tmp[query_labels[i] == 255] = 255\n",
        "        for j in range(n_ways):\n",
        "            query_label_tmp[query_labels[i] == class_ids[j]] = j + 1\n",
        "\n",
        "    ###### Generate query mask for each semantic class (including BG) ######\n",
        "    # BG class\n",
        "    query_masks = [[torch.where(query_label == 0,\n",
        "                                torch.ones_like(query_label),\n",
        "                                torch.zeros_like(query_label))[None, ...],]\n",
        "                   for query_label in query_labels]\n",
        "    # Other classes in query image\n",
        "    for i, query_label in enumerate(query_labels):\n",
        "        for idx in query_cls_idx[i][1:]:\n",
        "            mask = torch.where(query_label == class_ids[idx - 1],\n",
        "                               torch.ones_like(query_label),\n",
        "                               torch.zeros_like(query_label))[None, ...]\n",
        "            query_masks[i].append(mask)\n",
        "\n",
        "\n",
        "    return {'class_ids': class_ids,\n",
        "\n",
        "            'support_images_t': support_images_t,\n",
        "            'support_images': support_images,\n",
        "            'support_mask': support_mask,\n",
        "            'support_inst': support_insts,\n",
        "\n",
        "            'query_images_t': query_images_t,\n",
        "            'query_images': query_images,\n",
        "            'query_labels': query_labels_tmp,\n",
        "            'query_masks': query_masks,\n",
        "            'query_cls_idx': query_cls_idx,\n",
        "           }\n",
        "\n",
        "\n",
        "def voc_fewshot(base_dir, split, transforms, to_tensor, labels, n_ways, n_shots, max_iters,\n",
        "                n_queries=1):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        base_dir:\n",
        "            VOC dataset directory\n",
        "        split:\n",
        "            which split to use\n",
        "            choose from ('train', 'val', 'trainval', 'trainaug')\n",
        "        transform:\n",
        "            transformations to be performed on images/masks\n",
        "        to_tensor:\n",
        "            transformation to convert PIL Image to tensor\n",
        "        labels:\n",
        "            object class labels of the data\n",
        "        n_ways:\n",
        "            n-way few-shot learning, should be no more than # of object class labels\n",
        "        n_shots:\n",
        "            n-shot few-shot learning\n",
        "        max_iters:\n",
        "            number of pairs\n",
        "        n_queries:\n",
        "            number of query images\n",
        "    \"\"\"\n",
        "    voc = VOC(base_dir=base_dir, split=split, transforms=transforms, to_tensor=to_tensor)\n",
        "    voc.add_attrib('basic', attrib_basic, {})\n",
        "\n",
        "    # Load image ids for each class\n",
        "    sub_ids = []\n",
        "    for label in labels:\n",
        "        with open(os.path.join(voc._id_dir, voc.split,\n",
        "                               'class{}.txt'.format(label)), 'r') as f:\n",
        "            sub_ids.append(f.read().splitlines())\n",
        "    # Create sub-datasets and add class_id attribute\n",
        "    subsets = voc.subsets(sub_ids, [{'basic': {'class_id': cls_id}} for cls_id in labels])\n",
        "\n",
        "    # Choose the classes of queries\n",
        "    cnt_query = np.bincount(random.choices(population=range(n_ways), k=n_queries), minlength=n_ways)\n",
        "    # Set the number of images for each class\n",
        "    n_elements = [n_shots + x for x in cnt_query]\n",
        "    # Create paired dataset\n",
        "    paired_data = PairedDataset(subsets, n_elements=n_elements, max_iters=max_iters, same=False,\n",
        "                                pair_based_transforms=[\n",
        "                                    (fewShot, {'n_ways': n_ways, 'n_shots': n_shots,\n",
        "                                               'cnt_query': cnt_query})])\n",
        "    return paired_data\n",
        "\n",
        "\n",
        "def coco_fewshot(base_dir, split, transforms, to_tensor, labels, n_ways, n_shots, max_iters,\n",
        "                 n_queries=1):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        base_dir:\n",
        "            COCO dataset directory\n",
        "        split:\n",
        "            which split to use\n",
        "            choose from ('train', 'val')\n",
        "        transform:\n",
        "            transformations to be performed on images/masks\n",
        "        to_tensor:\n",
        "            transformation to convert PIL Image to tensor\n",
        "        labels:\n",
        "            labels of the data\n",
        "        n_ways:\n",
        "            n-way few-shot learning, should be no more than # of labels\n",
        "        n_shots:\n",
        "            n-shot few-shot learning\n",
        "        max_iters:\n",
        "            number of pairs\n",
        "        n_queries:\n",
        "            number of query images\n",
        "    \"\"\"\n",
        "    cocoseg = COCOSeg(base_dir, split, transforms, to_tensor)\n",
        "    cocoseg.add_attrib('basic', attrib_basic, {})\n",
        "\n",
        "    # Load image ids for each class\n",
        "    cat_ids = cocoseg.coco.getCatIds()\n",
        "    sub_ids = [cocoseg.coco.getImgIds(catIds=cat_ids[i - 1]) for i in labels]\n",
        "    # Create sub-datasets and add class_id attribute\n",
        "    subsets = cocoseg.subsets(sub_ids, [{'basic': {'class_id': cat_ids[i - 1]}} for i in labels])\n",
        "\n",
        "    # Choose the classes of queries\n",
        "    cnt_query = np.bincount(random.choices(population=range(n_ways), k=n_queries),\n",
        "                            minlength=n_ways)\n",
        "    # Set the number of images for each class\n",
        "    n_elements = [n_shots + x for x in cnt_query]\n",
        "    # Create paired dataset\n",
        "    paired_data = PairedDataset(subsets, n_elements=n_elements, max_iters=max_iters, same=False,\n",
        "                                pair_based_transforms=[\n",
        "                                    (fewShot, {'n_ways': n_ways, 'n_shots': n_shots,\n",
        "                                               'cnt_query': cnt_query, 'coco': True})])\n",
        "    return paired_data\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6a5lymyMSpW"
      },
      "source": [
        "**Encoder of features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIuTl2iAfpny"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder for few shot segmentation\n",
        "\n",
        "    Args:\n",
        "        in_channels:\n",
        "            number of input channels\n",
        "        pretrained_path:\n",
        "            path of the model for initialization\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, pretrained_path=None):\n",
        "        super().__init__()\n",
        "        self.pretrained_path = pretrained_path\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            self._make_layer(2, in_channels, 64),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            self._make_layer(2, 64, 128),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            self._make_layer(3, 128, 256),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            self._make_layer(3, 256, 512),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            self._make_layer(3, 512, 512, dilation=2, lastRelu=False),\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "    def _make_layer(self, n_convs, in_channels, out_channels, dilation=1, lastRelu=True):\n",
        "        \"\"\"\n",
        "        Make a (conv, relu) layer\n",
        "\n",
        "        Args:\n",
        "            n_convs:\n",
        "                number of convolution layers\n",
        "            in_channels:\n",
        "                input channels\n",
        "            out_channels:\n",
        "                output channels\n",
        "        \"\"\"\n",
        "        layer = []\n",
        "        for i in range(n_convs):\n",
        "            layer.append(nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                                   dilation=dilation, padding=dilation))\n",
        "            if i != n_convs - 1 or lastRelu:\n",
        "                layer.append(nn.ReLU(inplace=True))\n",
        "            in_channels = out_channels\n",
        "        return nn.Sequential(*layer)\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "\n",
        "        if self.pretrained_path is not None:\n",
        "            dic = torch.load(self.pretrained_path, map_location='cpu')\n",
        "            keys = list(dic.keys())\n",
        "            new_dic = self.state_dict()\n",
        "            new_keys = list(new_dic.keys())\n",
        "\n",
        "            for i in range(26):\n",
        "                new_dic[new_keys[i]] = dic[keys[i]]\n",
        "\n",
        "            self.load_state_dict(new_dic)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NioZL6dYMW8_"
      },
      "source": [
        "**Model Definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wPMLfN9IPpP"
      },
      "source": [
        "if mode == 'train':\n",
        "  align = True\n",
        "\n",
        "class FewShotSeg(nn.Module):\n",
        "    \"\"\"\n",
        "    Fewshot Segmentation model\n",
        "\n",
        "    Args:\n",
        "        in_channels:\n",
        "            number of input channels\n",
        "        pretrained_path:\n",
        "            path of the model for initialization\n",
        "        cfg:\n",
        "            model configurations\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, pretrained_path=None, cfg=False):\n",
        "        super().__init__()\n",
        "        self.pretrained_path = pretrained_path\n",
        "        self.config = cfg or {align : False}\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(OrderedDict([\n",
        "            ('backbone', Encoder(in_channels, self.pretrained_path)),]))\n",
        "\n",
        "\n",
        "    def forward(self, supp_imgs, fore_mask, back_mask, qry_imgs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            supp_imgs: support images\n",
        "                way x shot x [B x 3 x H x W], list of lists of tensors\n",
        "            fore_mask: foreground masks for support images\n",
        "                way x shot x [B x H x W], list of lists of tensors\n",
        "            back_mask: background masks for support images\n",
        "                way x shot x [B x H x W], list of lists of tensors\n",
        "            qry_imgs: query images\n",
        "                N x [B x 3 x H x W], list of tensors\n",
        "        \"\"\"\n",
        "        n_ways = len(supp_imgs)\n",
        "        n_shots = len(supp_imgs[0])\n",
        "        n_queries = len(qry_imgs)\n",
        "        batch_size = supp_imgs[0][0].shape[0]\n",
        "        img_size = supp_imgs[0][0].shape[-2:]\n",
        "\n",
        "        ###### Extract features ######\n",
        "        imgs_concat = torch.cat([torch.cat(way, dim=0) for way in supp_imgs]\n",
        "                                + [torch.cat(qry_imgs, dim=0),], dim=0)\n",
        "        img_fts = self.encoder(imgs_concat)\n",
        "        fts_size = img_fts.shape[-2:]\n",
        "\n",
        "        supp_fts = img_fts[:n_ways * n_shots * batch_size].view(\n",
        "            n_ways, n_shots, batch_size, -1, *fts_size)  # Wa x Sh x B x C x H' x W'\n",
        "        qry_fts = img_fts[n_ways * n_shots * batch_size:].view(\n",
        "            n_queries, batch_size, -1, *fts_size)   # N x B x C x H' x W'\n",
        "        fore_mask = torch.stack([torch.stack(way, dim=0)\n",
        "                                 for way in fore_mask], dim=0)  # Wa x Sh x B x H x W\n",
        "        back_mask = torch.stack([torch.stack(way, dim=0)\n",
        "                                 for way in back_mask], dim=0)  # Wa x Sh x B x H x W\n",
        "\n",
        "        ###### Compute loss ######\n",
        "        align_loss = 0\n",
        "        outputs = []\n",
        "        for epi in range(batch_size):\n",
        "            ###### Extract prototype ######\n",
        "            supp_fg_fts = [[self.getFeatures(supp_fts[way, shot, [epi]],\n",
        "                                             fore_mask[way, shot, [epi]])\n",
        "                            for shot in range(n_shots)] for way in range(n_ways)]\n",
        "            supp_bg_fts = [[self.getFeatures(supp_fts[way, shot, [epi]],\n",
        "                                             back_mask[way, shot, [epi]])\n",
        "                            for shot in range(n_shots)] for way in range(n_ways)]\n",
        "\n",
        "            ###### Obtain the prototypes######\n",
        "            fg_prototypes, bg_prototype = self.getPrototype(supp_fg_fts, supp_bg_fts)\n",
        "\n",
        "            ###### Compute the distance ######\n",
        "            prototypes = [bg_prototype,] + fg_prototypes\n",
        "            dist = [self.calDist(qry_fts[:, epi], prototype) for prototype in prototypes]\n",
        "            pred = torch.stack(dist, dim=1)  # N x (1 + Wa) x H' x W'\n",
        "            outputs.append(F.interpolate(pred, size=img_size, mode='bilinear'))\n",
        "\n",
        "            ###### Prototype alignment loss ######\n",
        "            if self.config and self.training:\n",
        "                align_loss_epi = self.alignLoss(qry_fts[:, epi], pred, supp_fts[:, :, epi],\n",
        "                                                fore_mask[:, :, epi], back_mask[:, :, epi])\n",
        "                align_loss += align_loss_epi\n",
        "\n",
        "        output = torch.stack(outputs, dim=1)  # N x B x (1 + Wa) x H x W\n",
        "        output = output.view(-1, *output.shape[2:])\n",
        "        return output, align_loss / batch_size\n",
        "\n",
        "\n",
        "    def calDist(self, fts, prototype, scaler=20):\n",
        "        \"\"\"\n",
        "        Calculate the distance between features and prototypes\n",
        "\n",
        "        Args:\n",
        "            fts: input features\n",
        "                expect shape: N x C x H' x W'\n",
        "            prototype: prototype of one semantic class\n",
        "                expect shape: 1 x C\n",
        "        \"\"\"\n",
        "        dist = F.cosine_similarity(fts, prototype[..., None, None], dim=1) * scaler\n",
        "        return dist\n",
        "\n",
        "\n",
        "    def getFeatures(self, fts, mask):\n",
        "        \"\"\"\n",
        "        Extract foreground and background features via masked average pooling\n",
        "\n",
        "        Args:\n",
        "            fts: input features, expect shape: 1 x C x H' x W'\n",
        "            mask: binary mask, expect shape: 1 x H x W\n",
        "        \"\"\"\n",
        "        fts = F.interpolate(fts, size=mask.shape[-2:], mode='bilinear')\n",
        "        masked_fts = torch.sum(fts * mask[None, ...], dim=(2, 3)) \\\n",
        "            / (mask[None, ...].sum(dim=(2, 3)) + 1e-5) # 1 x C\n",
        "        return masked_fts\n",
        "\n",
        "\n",
        "    def getPrototype(self, fg_fts, bg_fts):\n",
        "        \"\"\"\n",
        "        Average the features to obtain the prototype\n",
        "\n",
        "        Args:\n",
        "            fg_fts: lists of list of foreground features for each way/shot\n",
        "                expect shape: Wa x Sh x [1 x C]\n",
        "            bg_fts: lists of list of background features for each way/shot\n",
        "                expect shape: Wa x Sh x [1 x C]\n",
        "        \"\"\"\n",
        "        n_ways, n_shots = len(fg_fts), len(fg_fts[0])\n",
        "        fg_prototypes = [sum(way) / n_shots for way in fg_fts]\n",
        "        bg_prototype = sum([sum(way) / n_shots for way in bg_fts]) / n_ways\n",
        "        return fg_prototypes, bg_prototype\n",
        "\n",
        "\n",
        "    def alignLoss(self, qry_fts, pred, supp_fts, fore_mask, back_mask):\n",
        "        \"\"\"\n",
        "        Compute the loss for the prototype alignment branch\n",
        "\n",
        "        Args:\n",
        "            qry_fts: embedding features for query images\n",
        "                expect shape: N x C x H' x W'\n",
        "            pred: predicted segmentation score\n",
        "                expect shape: N x (1 + Wa) x H x W\n",
        "            supp_fts: embedding features for support images\n",
        "                expect shape: Wa x Sh x C x H' x W'\n",
        "            fore_mask: foreground masks for support images\n",
        "                expect shape: way x shot x H x W\n",
        "            back_mask: background masks for support images\n",
        "                expect shape: way x shot x H x W\n",
        "        \"\"\"\n",
        "        n_ways, n_shots = len(fore_mask), len(fore_mask[0])\n",
        "\n",
        "        # Mask and get query prototype\n",
        "        pred_mask = pred.argmax(dim=1, keepdim=True)  # N x 1 x H' x W'\n",
        "        binary_masks = [pred_mask == i for i in range(1 + n_ways)]\n",
        "        skip_ways = [i for i in range(n_ways) if binary_masks[i + 1].sum() == 0]\n",
        "        pred_mask = torch.stack(binary_masks, dim=1).float()  # N x (1 + Wa) x 1 x H' x W'\n",
        "        qry_prototypes = torch.sum(qry_fts.unsqueeze(1) * pred_mask, dim=(0, 3, 4))\n",
        "        qry_prototypes = qry_prototypes / (pred_mask.sum((0, 3, 4)) + 1e-5)  # (1 + Wa) x C\n",
        "\n",
        "        # Compute the support loss\n",
        "        loss = 0\n",
        "        for way in range(n_ways):\n",
        "            if way in skip_ways:\n",
        "                continue\n",
        "            # Get the query prototypes\n",
        "            prototypes = [qry_prototypes[[0]], qry_prototypes[[way + 1]]]\n",
        "            for shot in range(n_shots):\n",
        "                img_fts = supp_fts[way, [shot]]\n",
        "                supp_dist = [self.calDist(img_fts, prototype) for prototype in prototypes]\n",
        "                supp_pred = torch.stack(supp_dist, dim=1)\n",
        "                supp_pred = F.interpolate(supp_pred, size=fore_mask.shape[-2:],\n",
        "                                          mode='bilinear')\n",
        "                # Construct the support Ground-Truth segmentation\n",
        "                supp_label = torch.full_like(fore_mask[way, shot], 255,\n",
        "                                             device=img_fts.device).long()\n",
        "                supp_label[fore_mask[way, shot] == 1] = 1\n",
        "                supp_label[back_mask[way, shot] == 1] = 0\n",
        "                # Compute Loss\n",
        "                loss = loss + F.cross_entropy(\n",
        "                    supp_pred, supp_label[None, ...], ignore_index=255) / n_shots / n_ways\n",
        "        return loss"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fMp4A-o0o8T"
      },
      "source": [
        "#import torch\n",
        "#import numpy as np\n",
        "\n",
        "#print(abc[0])\n",
        "#abcd=torch.from_numpy(abc)\n",
        "#print(abcd)\n",
        "#for way in abcd:\n",
        "#  print(way)\n",
        "\n",
        "#abcd2=[torch.cat(way,dim=0) for way in abcd]\n",
        "#print(abcd2)\n",
        "\n",
        "#a=torch.tensor([[1,2],[1,2]])\n",
        "#b=torch.tensor([[3,4],[3,4]])\n",
        "#abcd3 = torch.stack((a,b), dim=0)\n",
        "#print(a)\n",
        "#print(abcd3)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH_GNQuPMbMM"
      },
      "source": [
        "**Train script**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reZkWyhV7ZY1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a98333c6-c1d5-4834-cae6-56a508cb1750"
      },
      "source": [
        "base_directory='/content/drive/My Drive/IITB_assignments/CV_Project/VOC_TrainingSet/VOCdevkit/VOC2012'\n",
        "\n",
        "#@ex.automain\n",
        "\n",
        "#if _run.observers:\n",
        "#    os.makedirs(f'{_run.observers[0].dir}/snapshots', exist_ok=True)\n",
        "#    for source_file, _ in _run.experiment_info['sources']:\n",
        "#        os.makedirs(os.path.dirname(f'{_run.observers[0].dir}/source/{source_file}'),\n",
        "#                    exist_ok=True)\n",
        "#        _run.observers[0].save_file(source_file, f'source/{source_file}')\n",
        "#    shutil.rmtree(f'{_run.observers[0].basedir}/_sources')\n",
        "\n",
        "seed=0\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "cudnn.enabled = True\n",
        "cudnn.benchmark = True\n",
        "torch.cuda.set_device(device=0)\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "\n",
        "###### Create model ######\n",
        "model = FewShotSeg(pretrained_path=base_directory+'/vgg16-pretrained.pth', cfg=True)\n",
        "model = nn.DataParallel(model.cuda(), device_ids=None)\n",
        "model.train()\n",
        "\n",
        "\n",
        "###### Load data ######\n",
        "\n",
        "make_data = voc_fewshot\n",
        "labels = set(range(1, 21)) - set(range(1, 6))\n",
        "transforms = Compose([Resize(size=(417, 417)),\n",
        "                      RandomMirror()])\n",
        "dataset = make_data(\n",
        "    base_dir=base_directory,\n",
        "    split='trainaug',\n",
        "    transforms=transforms,\n",
        "    to_tensor=ToTensorNormalize(),\n",
        "    labels=labels,\n",
        "    max_iters=30000,                               # 30000\n",
        "    n_ways=1,\n",
        "    n_shots=5,\n",
        "    n_queries=1\n",
        ")\n",
        "trainloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    drop_last=True\n",
        ")\n",
        "print('Length' , len(trainloader))\n",
        "#_log.info('###### Set optimizer ######')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.0005)\n",
        "scheduler = MultiStepLR(optimizer, milestones=[100, 200, 300], gamma=0.1)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "i_iter = 0\n",
        "log_loss = {'loss': 0, 'align_loss': 0}\n",
        "\n",
        "#_log.info('###### Training ######')\n",
        "for i_iter, sample_batched in enumerate(trainloader):\n",
        "    # Prepare input\n",
        "    support_images = [[shot.cuda() for shot in way]\n",
        "                      for way in sample_batched['support_images']]\n",
        "    support_fg_mask = [[shot[f'fg_mask'].float().cuda() for shot in way]\n",
        "                        for way in sample_batched['support_mask']]\n",
        "    support_bg_mask = [[shot[f'bg_mask'].float().cuda() for shot in way]\n",
        "                        for way in sample_batched['support_mask']]\n",
        "\n",
        "    query_images = [query_image.cuda()\n",
        "                    for query_image in sample_batched['query_images']]\n",
        "    query_labels = torch.cat(\n",
        "        [query_label.long().cuda() for query_label in sample_batched['query_labels']], dim=0)\n",
        "\n",
        "    # Forward and Backward\n",
        "    optimizer.zero_grad()\n",
        "    query_pred, align_loss = model(support_images, support_fg_mask, support_bg_mask,\n",
        "                                    query_images)\n",
        "    query_loss = criterion(query_pred, query_labels)\n",
        "    loss = query_loss + align_loss * 1\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    # Log loss\n",
        "    query_loss = query_loss.detach().data.cpu().numpy()\n",
        "    align_loss = align_loss.detach().data.cpu().numpy() if align_loss != 0 else 0\n",
        "    #_run.log_scalar('loss', query_loss)\n",
        "    #_run.log_scalar('align_loss', align_loss)\n",
        "    log_loss['loss'] += query_loss\n",
        "    log_loss['align_loss'] += align_loss\n",
        "\n",
        "    # print loss and take snapshots\n",
        "    if (i_iter + 1) % 5 == 0:\n",
        "        loss = log_loss['loss'] / (i_iter + 1)\n",
        "        align_loss = log_loss['align_loss'] / (i_iter + 1)\n",
        "        print(f'step {i_iter+1}: loss: {loss}, align_loss: {align_loss}')\n",
        "\n",
        " #   if (i_iter + 1) % 5 == 0:\n",
        " #       _log.info('###### Taking snapshot ######')\n",
        "#        torch.save(model.state_dict(),\n",
        "#                    os.path.join(f'{_run.observers[0].dir}/snapshots', f'{i_iter + 1}.pth'))\n",
        "\n",
        "#_log.info('###### Saving final model ######')\n",
        "torch.save(model.state_dict(),\n",
        "            os.path.join(base_directory, f'{i_iter + 1}.pth'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length 937\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBDZqvAcGqzz"
      },
      "source": [
        "# from IPython.display import display, Image\n",
        "# display(Image(filename='/content/drive/My Drive/IITB_assignments/CV_Project/VOC_TrainingSet/VOCdevkit/VOC2012/JPEGImages/2008_001866.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}